{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "礙於不是所有同學都有 GPU ，這邊的範例使用的是簡化版本的 ResNet，確保所有同學都能夠順利訓練!\n",
    "\n",
    "\n",
    "最後一天的作業請閱讀這篇非常詳盡的[文章](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)，基本上已經涵蓋了所有訓練　CNN 常用的技巧，請使用所有學過的訓練技巧，盡可能地提高 Cifar-10 的 test data 準確率，截圖你最佳的結果並上傳來完成最後一次的作業吧!\n",
    "\n",
    "另外這些技巧在 Kaggle 上也會被許多人使用，更有人會開發一些新的技巧，例如使把預訓練在 ImageNet 上的模型當成 feature extractor 後，再拿擷取出的特徵重新訓練新的模型，這些技巧再進階的課程我們會在提到，有興趣的同學也可以[參考](https://www.kaggle.com/insaff/img-feature-extraction-with-pretrained-resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10, cifar100\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv2D, Dense, Input, add, Activation, GlobalAveragePooling2D\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "from keras.models import Model\n",
    "from keras import optimizers, regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "#from tensorflow.python.keras.layers import Flatten, Dropout\n",
    "#from tensorflow.python.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "subtract_pixel_mean = True\n",
    "\n",
    "# 讀取資料集並作前處理\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data. # 将数据格式化到0~1\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "    \n",
    "# Convert class vectors to binary class matrices.\n",
    "# 进行one-hot编码\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_n            = 1\n",
    "layers             = 6 * stack_n + 2\n",
    "num_classes        = 10\n",
    "batch_size         = 128\n",
    "epochs             = 100\n",
    "iterations         = 10000 // batch_size + 1\n",
    "weight_decay       = 1e-4\n",
    "\n",
    "log_filepath = './my_resnet_32/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 81:\n",
    "        return 0.1\n",
    "    if epoch < 122:\n",
    "        return 0.01\n",
    "    return 0.001\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, 10) # number of classes\n",
    "y_test = keras.utils.to_categorical(y_test,10)# number of classes\n",
    "\n",
    "# color preprocessing\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_block(x,o_filters,increase=False):\n",
    "    stride = (1,1)\n",
    "    if increase:\n",
    "        stride = (2,2)\n",
    "        \n",
    "    conv_1 = Conv2D(o_filters,kernel_size=(3,3),strides=stride,padding='same',\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    \n",
    "    o1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(conv_1))\n",
    "    \n",
    "    conv_2 = Conv2D(o_filters,kernel_size=(3,3),strides=(1,1),padding='same',\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay))(o1)\n",
    "    \n",
    "    o2  = BatchNormalization(momentum=0.9, epsilon=1e-5)(conv_2)\n",
    "\n",
    "    if increase:\n",
    "        projection = Conv2D(o_filters,kernel_size=(1,1),strides=(2,2),padding='same',\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "        block = add([o2, projection])\n",
    "        block = Activation('relu')(block)\n",
    "    else:\n",
    "        block = add([o2, x])\n",
    "        block = Activation('relu')(block)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_network(img_input,classes_num=10,stack_n=5):\n",
    "    # build model ( total layers = stack_n * 3 * 2 + 2 )\n",
    "    # stack_n = 5 by default, total layers = 32\n",
    "    # input: 32x32x3 output: 32x32x16\n",
    "    x = Conv2D(filters=16,kernel_size=(3,3),strides=(1,1),padding='same',\n",
    "               kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "\n",
    "    # input: 32x32x16 output: 32x32x16\n",
    "    for _ in range(stack_n):\n",
    "        x = residual_block(x,16,False)\n",
    "\n",
    "    # input: 32x32x16 output: 16x16x32\n",
    "    x = residual_block(x,32,True)\n",
    "    for _ in range(1,stack_n):\n",
    "        x = residual_block(x,32,False)\n",
    "    \n",
    "    # input: 16x16x32 output: 8x8x64\n",
    "    x = residual_block(x,64,True)\n",
    "    for _ in range(1,stack_n):\n",
    "        x = residual_block(x,64,False)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # input: 64 output: 10\n",
    "    x = Dense(classes_num,activation='softmax',kernel_initializer=\"he_normal\",\n",
    "              kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 16)   448         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 16)   2320        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 32, 16)   64          conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 32, 16)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 16)   2320        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 32, 16)   64          conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 32, 32, 16)   0           batch_normalization_35[0][0]     \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 32, 16)   0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 32)   4640        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 32)   128         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16, 16, 32)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 32)   9248        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 32)   128         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 32)   544         activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 16, 16, 32)   0           batch_normalization_37[0][0]     \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16, 16, 32)   0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 64)     18496       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 64)     256         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 64)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 64)     36928       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 64)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 64)     2112        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 8, 8, 64)     0           batch_normalization_39[0][0]     \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 64)     0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 8, 64)     256         activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 64)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 64)           0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           650         global_average_pooling2d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 78,858\n",
      "Trainable params: 78,282\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build network\n",
    "img_input = Input(shape=(32,32,3))\n",
    "output    = residual_network(img_input,10,stack_n)  # 5\n",
    "model     = Model(img_input, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "79/79 [==============================] - 126s 2s/step - loss: 1.7568 - acc: 0.3685 - val_loss: 1.8591 - val_acc: 0.3727\n",
      "Epoch 2/100\n",
      "79/79 [==============================] - 121s 2s/step - loss: 1.5620 - acc: 0.4454 - val_loss: 1.4812 - val_acc: 0.4862\n",
      "Epoch 3/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 1.3735 - acc: 0.5262 - val_loss: 1.3914 - val_acc: 0.5322\n",
      "Epoch 4/100\n",
      "79/79 [==============================] - 119s 2s/step - loss: 1.2554 - acc: 0.5726 - val_loss: 1.3157 - val_acc: 0.5498\n",
      "Epoch 5/100\n",
      "79/79 [==============================] - 119s 2s/step - loss: 1.2034 - acc: 0.5979 - val_loss: 1.2993 - val_acc: 0.5750\n",
      "Epoch 6/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 1.1405 - acc: 0.6183 - val_loss: 1.2881 - val_acc: 0.5731\n",
      "Epoch 7/100\n",
      "79/79 [==============================] - 122s 2s/step - loss: 1.1042 - acc: 0.6337 - val_loss: 1.2555 - val_acc: 0.5831\n",
      "Epoch 8/100\n",
      "79/79 [==============================] - 121s 2s/step - loss: 1.0947 - acc: 0.6341 - val_loss: 1.2544 - val_acc: 0.5962\n",
      "Epoch 9/100\n",
      "79/79 [==============================] - 118s 1s/step - loss: 1.0438 - acc: 0.6576 - val_loss: 1.0766 - val_acc: 0.6505\n",
      "Epoch 10/100\n",
      "79/79 [==============================] - 119s 2s/step - loss: 1.0086 - acc: 0.6726 - val_loss: 1.1529 - val_acc: 0.6288\n",
      "Epoch 11/100\n",
      "79/79 [==============================] - 134s 2s/step - loss: 0.9878 - acc: 0.6749 - val_loss: 1.1456 - val_acc: 0.6334\n",
      "Epoch 12/100\n",
      "79/79 [==============================] - 135s 2s/step - loss: 0.9624 - acc: 0.6888 - val_loss: 0.9736 - val_acc: 0.6862\n",
      "Epoch 13/100\n",
      "79/79 [==============================] - 130s 2s/step - loss: 0.9591 - acc: 0.6949 - val_loss: 0.9857 - val_acc: 0.6875\n",
      "Epoch 14/100\n",
      "79/79 [==============================] - 125s 2s/step - loss: 0.9277 - acc: 0.6993 - val_loss: 1.0397 - val_acc: 0.6631\n",
      "Epoch 15/100\n",
      "79/79 [==============================] - 122s 2s/step - loss: 0.9243 - acc: 0.7082 - val_loss: 1.0605 - val_acc: 0.6805\n",
      "Epoch 16/100\n",
      "79/79 [==============================] - 130s 2s/step - loss: 0.8978 - acc: 0.7139 - val_loss: 0.9871 - val_acc: 0.6845\n",
      "Epoch 17/100\n",
      "79/79 [==============================] - 122s 2s/step - loss: 0.8676 - acc: 0.7311 - val_loss: 0.9965 - val_acc: 0.6882\n",
      "Epoch 18/100\n",
      "79/79 [==============================] - 115s 1s/step - loss: 0.8700 - acc: 0.7262 - val_loss: 0.9608 - val_acc: 0.7101\n",
      "Epoch 19/100\n",
      "79/79 [==============================] - 123s 2s/step - loss: 0.8756 - acc: 0.7300 - val_loss: 0.9879 - val_acc: 0.7006\n",
      "Epoch 20/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.8403 - acc: 0.7422 - val_loss: 0.9379 - val_acc: 0.7154\n",
      "Epoch 21/100\n",
      "79/79 [==============================] - 122s 2s/step - loss: 0.8403 - acc: 0.7426 - val_loss: 0.9723 - val_acc: 0.7095\n",
      "Epoch 22/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.8271 - acc: 0.7493 - val_loss: 0.8650 - val_acc: 0.7394\n",
      "Epoch 23/100\n",
      "79/79 [==============================] - 126s 2s/step - loss: 0.8098 - acc: 0.7549 - val_loss: 1.0755 - val_acc: 0.6704\n",
      "Epoch 24/100\n",
      "79/79 [==============================] - 125s 2s/step - loss: 0.8168 - acc: 0.7485 - val_loss: 1.0193 - val_acc: 0.6976\n",
      "Epoch 25/100\n",
      "79/79 [==============================] - 123s 2s/step - loss: 0.7872 - acc: 0.7633 - val_loss: 1.0184 - val_acc: 0.7003\n",
      "Epoch 26/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7836 - acc: 0.7703 - val_loss: 0.8711 - val_acc: 0.7451\n",
      "Epoch 27/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7942 - acc: 0.7588 - val_loss: 0.7984 - val_acc: 0.7672\n",
      "Epoch 28/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7803 - acc: 0.7711 - val_loss: 0.9647 - val_acc: 0.7145\n",
      "Epoch 29/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7890 - acc: 0.7629 - val_loss: 0.9232 - val_acc: 0.7310\n",
      "Epoch 30/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7703 - acc: 0.7755 - val_loss: 0.7873 - val_acc: 0.7670\n",
      "Epoch 31/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7660 - acc: 0.7779 - val_loss: 0.8532 - val_acc: 0.7539\n",
      "Epoch 32/100\n",
      "79/79 [==============================] - 121s 2s/step - loss: 0.7659 - acc: 0.7772 - val_loss: 0.8219 - val_acc: 0.7606\n",
      "Epoch 33/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7628 - acc: 0.7753 - val_loss: 0.7689 - val_acc: 0.7723\n",
      "Epoch 34/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7567 - acc: 0.7827 - val_loss: 0.8956 - val_acc: 0.7478\n",
      "Epoch 35/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7541 - acc: 0.7801 - val_loss: 0.8093 - val_acc: 0.7691\n",
      "Epoch 36/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7401 - acc: 0.7848 - val_loss: 0.8342 - val_acc: 0.7531\n",
      "Epoch 37/100\n",
      "79/79 [==============================] - 131s 2s/step - loss: 0.7371 - acc: 0.7851 - val_loss: 0.8043 - val_acc: 0.7693\n",
      "Epoch 38/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.7344 - acc: 0.7916 - val_loss: 0.8019 - val_acc: 0.7715\n",
      "Epoch 39/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.7279 - acc: 0.7931 - val_loss: 0.8115 - val_acc: 0.7665\n",
      "Epoch 40/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.7288 - acc: 0.7911 - val_loss: 0.8401 - val_acc: 0.7626\n",
      "Epoch 41/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.7140 - acc: 0.7990 - val_loss: 0.8323 - val_acc: 0.7653\n",
      "Epoch 42/100\n",
      "79/79 [==============================] - 123s 2s/step - loss: 0.7313 - acc: 0.7869 - val_loss: 0.7389 - val_acc: 0.7907\n",
      "Epoch 43/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7123 - acc: 0.7977 - val_loss: 0.8151 - val_acc: 0.7679\n",
      "Epoch 44/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7333 - acc: 0.7914 - val_loss: 0.7787 - val_acc: 0.7827\n",
      "Epoch 45/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7075 - acc: 0.7983 - val_loss: 0.9144 - val_acc: 0.7403\n",
      "Epoch 46/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7116 - acc: 0.7962 - val_loss: 0.8512 - val_acc: 0.7511\n",
      "Epoch 47/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7094 - acc: 0.7994 - val_loss: 0.7767 - val_acc: 0.7833\n",
      "Epoch 48/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7348 - acc: 0.7957 - val_loss: 0.8027 - val_acc: 0.7705\n",
      "Epoch 49/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7135 - acc: 0.8008 - val_loss: 0.7918 - val_acc: 0.7801\n",
      "Epoch 50/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.6848 - acc: 0.8070 - val_loss: 0.7794 - val_acc: 0.7787\n",
      "Epoch 51/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.6919 - acc: 0.8079 - val_loss: 0.7858 - val_acc: 0.7804\n",
      "Epoch 52/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7162 - acc: 0.7994 - val_loss: 0.7251 - val_acc: 0.7967\n",
      "Epoch 53/100\n",
      "79/79 [==============================] - 121s 2s/step - loss: 0.7137 - acc: 0.7972 - val_loss: 0.7757 - val_acc: 0.7835\n",
      "Epoch 54/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.7140 - acc: 0.7946 - val_loss: 0.8131 - val_acc: 0.7826\n",
      "Epoch 55/100\n",
      "79/79 [==============================] - 119s 2s/step - loss: 0.6802 - acc: 0.8136 - val_loss: 0.7600 - val_acc: 0.7902\n",
      "Epoch 56/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.6914 - acc: 0.8094 - val_loss: 0.7624 - val_acc: 0.7926\n",
      "Epoch 57/100\n",
      "79/79 [==============================] - 119s 2s/step - loss: 0.7000 - acc: 0.8048 - val_loss: 0.8794 - val_acc: 0.7482\n",
      "Epoch 58/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.6961 - acc: 0.8075 - val_loss: 0.7917 - val_acc: 0.7912\n",
      "Epoch 59/100\n",
      "79/79 [==============================] - 121s 2s/step - loss: 0.6877 - acc: 0.8118 - val_loss: 0.7372 - val_acc: 0.7956\n",
      "Epoch 60/100\n",
      "79/79 [==============================] - 121s 2s/step - loss: 0.6872 - acc: 0.8083 - val_loss: 0.7928 - val_acc: 0.7819\n",
      "Epoch 61/100\n",
      "79/79 [==============================] - 121s 2s/step - loss: 0.6795 - acc: 0.8112 - val_loss: 0.8059 - val_acc: 0.7833\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 121s 2s/step - loss: 0.6943 - acc: 0.8080 - val_loss: 0.8379 - val_acc: 0.7705\n",
      "Epoch 63/100\n",
      "79/79 [==============================] - 130s 2s/step - loss: 0.6888 - acc: 0.8124 - val_loss: 0.8525 - val_acc: 0.7647\n",
      "Epoch 64/100\n",
      "79/79 [==============================] - 125s 2s/step - loss: 0.6817 - acc: 0.8129 - val_loss: 0.7951 - val_acc: 0.7794\n",
      "Epoch 65/100\n",
      "79/79 [==============================] - 125s 2s/step - loss: 0.6630 - acc: 0.8184 - val_loss: 0.7951 - val_acc: 0.7831\n",
      "Epoch 66/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.6838 - acc: 0.8137 - val_loss: 0.7543 - val_acc: 0.7956\n",
      "Epoch 67/100\n",
      "79/79 [==============================] - 127s 2s/step - loss: 0.6867 - acc: 0.8146 - val_loss: 0.7880 - val_acc: 0.7791\n",
      "Epoch 68/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.6785 - acc: 0.8154 - val_loss: 0.8108 - val_acc: 0.7757\n",
      "Epoch 69/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.6916 - acc: 0.8105 - val_loss: 0.7644 - val_acc: 0.7965\n",
      "Epoch 70/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.6712 - acc: 0.8163 - val_loss: 0.7473 - val_acc: 0.8034\n",
      "Epoch 71/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.6699 - acc: 0.8138 - val_loss: 0.8613 - val_acc: 0.7666\n",
      "Epoch 72/100\n",
      "79/79 [==============================] - 121s 2s/step - loss: 0.6730 - acc: 0.8126 - val_loss: 0.7236 - val_acc: 0.8079\n",
      "Epoch 73/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.6709 - acc: 0.8118 - val_loss: 0.7787 - val_acc: 0.7971\n",
      "Epoch 74/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.6661 - acc: 0.8188 - val_loss: 0.7344 - val_acc: 0.7984\n",
      "Epoch 75/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.6606 - acc: 0.8243 - val_loss: 0.7194 - val_acc: 0.8093\n",
      "Epoch 76/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.6599 - acc: 0.8192 - val_loss: 0.7970 - val_acc: 0.7778\n",
      "Epoch 77/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.6428 - acc: 0.8262 - val_loss: 0.7650 - val_acc: 0.7936\n",
      "Epoch 78/100\n",
      "79/79 [==============================] - 120s 2s/step - loss: 0.6580 - acc: 0.8239 - val_loss: 0.7563 - val_acc: 0.8008\n",
      "Epoch 79/100\n",
      "79/79 [==============================] - 135s 2s/step - loss: 0.6814 - acc: 0.8160 - val_loss: 0.7357 - val_acc: 0.8052\n",
      "Epoch 80/100\n",
      "79/79 [==============================] - 146s 2s/step - loss: 0.6471 - acc: 0.8287 - val_loss: 0.8833 - val_acc: 0.7700\n",
      "Epoch 81/100\n",
      "79/79 [==============================] - 126s 2s/step - loss: 0.6566 - acc: 0.8229 - val_loss: 0.7629 - val_acc: 0.7887\n",
      "Epoch 82/100\n",
      "79/79 [==============================] - 125s 2s/step - loss: 0.6720 - acc: 0.8231 - val_loss: 0.6443 - val_acc: 0.8323\n",
      "Epoch 83/100\n",
      "79/79 [==============================] - 123s 2s/step - loss: 0.5899 - acc: 0.8464 - val_loss: 0.6117 - val_acc: 0.8426\n",
      "Epoch 84/100\n",
      "79/79 [==============================] - 129s 2s/step - loss: 0.5798 - acc: 0.8481 - val_loss: 0.6070 - val_acc: 0.8426\n",
      "Epoch 85/100\n",
      "79/79 [==============================] - 127s 2s/step - loss: 0.5537 - acc: 0.8628 - val_loss: 0.6000 - val_acc: 0.8448\n",
      "Epoch 86/100\n",
      "79/79 [==============================] - 128s 2s/step - loss: 0.5430 - acc: 0.8612 - val_loss: 0.5949 - val_acc: 0.8494\n",
      "Epoch 87/100\n",
      "79/79 [==============================] - 126s 2s/step - loss: 0.5534 - acc: 0.8542 - val_loss: 0.5872 - val_acc: 0.8504\n",
      "Epoch 88/100\n",
      "79/79 [==============================] - 127s 2s/step - loss: 0.5342 - acc: 0.8630 - val_loss: 0.5869 - val_acc: 0.8509\n",
      "Epoch 89/100\n",
      "79/79 [==============================] - 123s 2s/step - loss: 0.5467 - acc: 0.8628 - val_loss: 0.5850 - val_acc: 0.8526\n",
      "Epoch 90/100\n",
      "79/79 [==============================] - 126s 2s/step - loss: 0.5369 - acc: 0.8634 - val_loss: 0.5806 - val_acc: 0.8552\n",
      "Epoch 91/100\n",
      "79/79 [==============================] - 126s 2s/step - loss: 0.5263 - acc: 0.8691 - val_loss: 0.5808 - val_acc: 0.8538\n",
      "Epoch 92/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.5237 - acc: 0.8711 - val_loss: 0.5801 - val_acc: 0.8571\n",
      "Epoch 93/100\n",
      "79/79 [==============================] - 130s 2s/step - loss: 0.5307 - acc: 0.8628 - val_loss: 0.5848 - val_acc: 0.8532\n",
      "Epoch 94/100\n",
      "79/79 [==============================] - 124s 2s/step - loss: 0.5336 - acc: 0.8648 - val_loss: 0.5727 - val_acc: 0.8553\n",
      "Epoch 95/100\n",
      "79/79 [==============================] - 125s 2s/step - loss: 0.5225 - acc: 0.8699 - val_loss: 0.5783 - val_acc: 0.8527\n",
      "Epoch 96/100\n",
      "79/79 [==============================] - 128s 2s/step - loss: 0.5178 - acc: 0.8738 - val_loss: 0.5752 - val_acc: 0.8534\n",
      "Epoch 97/100\n",
      "79/79 [==============================] - 123s 2s/step - loss: 0.5086 - acc: 0.8757 - val_loss: 0.5667 - val_acc: 0.8564\n",
      "Epoch 98/100\n",
      "79/79 [==============================] - 128s 2s/step - loss: 0.5152 - acc: 0.8726 - val_loss: 0.5749 - val_acc: 0.8535\n",
      "Epoch 99/100\n",
      "79/79 [==============================] - 131s 2s/step - loss: 0.5133 - acc: 0.8718 - val_loss: 0.5705 - val_acc: 0.8555\n",
      "Epoch 100/100\n",
      "79/79 [==============================] - 127s 2s/step - loss: 0.5090 - acc: 0.8727 - val_loss: 0.5718 - val_acc: 0.8555\n",
      "Test loss: 0.5717500943660736\n",
      "Test accuracy: 0.8555\n"
     ]
    }
   ],
   "source": [
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# set callback\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "cbks = [change_lr,tb_cb]\n",
    "\n",
    "# dump checkpoint if you need.(add it to cbks)\n",
    "# ModelCheckpoint('./checkpoint-{epoch}.h5', save_best_only=False, mode='auto', period=10)\n",
    "\n",
    "# set data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant',cval=0.)\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# start training\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                     steps_per_epoch=iterations,\n",
    "                     epochs=epochs,\n",
    "                     callbacks=cbks,\n",
    "                     validation_data=(x_test, y_test))\n",
    "model.save('my_resnet_32/my_resnet_32.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
